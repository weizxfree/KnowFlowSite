# KnowEval：RAG 工程化的最后一公里，让问答质量有据可依

## 前言

### 为什么需要 KnowEval？

在过去一年与众多企业客户的交流中，我们发现一个普遍的痛点：**RAG 系统上线后，如何量化评估问答质量？如何系统化提升检索效果？如何在多个优化方案中选择最优解？**

很多团队花费大量时间调试 RAG 系统，但往往凭感觉调参，缺乏数据支撑。A/B 测试需要人工逐条对比，效率低下。更关键的是，**没有一套标准化的评测体系，就无法形成可持续优化的闭环。**

基于此，我们推出了 **KnowEval - 专为 RAG 系统打造的全链路评测平台**，将评测这个"隐形能力"变成可视化、可量化、可优化的工程化能力。

![评估表盘](https://fastly.jsdelivr.net/gh/bucketio/img15@main/2025/11/10/1762763079527-25a4ef2a-0038-44a4-bc82-5e700da2cb3f.png)

### 新功能

KnowEval v1.0.0 正式发布，本版本推出以下核心能力：

**1. 五维度评测体系**

我们基于业界领先的 RAGAS 框架，构建了完整的 RAG 评测指标体系：

- **忠实度 (Faithfulness)**: 评估答案是否忠实于检索到的上下文，杜绝"幻觉"
- **答案正确性 (Answer Correctness)**: 对比标准答案，量化回答准确度
- **上下文精确度 (Context Precision)**: 评估检索到的文档是否精准相关
- **上下文召回率 (Context Recall)**: 评估是否检索到所有必要信息
- **答案相关性 (Answer Relevancy)**: 评估答案是否切题，避免答非所问

**2. AI 智能生成测试集**

手动构建测试数据集费时费力，我们提供了 **AI 自动生成测试集** 功能：

- 基于知识库内容自动生成问题
- 智能提取标准答案和参考上下文
- 支持自定义生成数量和难度
- 一键导入，即时可用

![数据集管理](https://fastly.jsdelivr.net/gh/bucketio/img0@main/2025/11/10/1762763115517-cd5f06ef-0b51-4aaa-b8e8-f602540295a4.png)

![AI 自动数据集](https://fastly.jsdelivr.net/gh/bucketio/img16@main/2025/11/10/1762763207505-b71ef0ee-c70a-474b-8eae-00fa78490b31.png)

**3. 可视化评测报告**

告别枯燥的数字表格，我们提供了直观的可视化报告：

- 多维度指标雷达图
- 得分分布趋势分析
- 单条问答详细诊断
- 导出 JSON 格式完整报告

![](https://fastly.jsdelivr.net/gh/bucketio/img7@main/2025/11/10/1762763180796-bf82e90c-87b8-4c1a-9d3a-dc9e8f553497.png)

**4. 对接 KnowFlow，形成完整闭环**

KnowEval 与 KnowFlow 深度集成，形成 **"数据治理 → RAG 检索 → 质量评测 → 持续优化"** 的完整闭环：

```
┌─────────────────────────────────────────────────────────┐
│                     KnowFlow 生态                        │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  ┌──────────────┐       ┌──────────────┐               │
│  │  KnowFlow    │──────▶│  RAG 应用    │               │
│  │  知识库平台   │       │  业务场景    │               │
│  └──────────────┘       └──────┬───────┘               │
│         │                      │                        │
│         │                      │                        │
│  ┌──────▼──────────────────────▼───────┐               │
│  │         KnowEval 评测平台            │               │
│  ├─────────────────────────────────────┤               │
│  │  • 五维度评测指标                    │               │
│  │  • AI 生成测试集                     │               │
│  │  • 可视化分析报告                    │               │
│  │  • A/B 测试对比                      │               │
│  └─────────────┬───────────────────────┘               │
│                │                                        │
│  ┌─────────────▼───────────────────────┐               │
│  │        数据驱动的优化决策             │               │
│  │  • 切块方法调优                       │               │
│  │  • 检索参数优化                       │               │
│  │  • Prompt 迭代改进                   │               │
│  └─────────────────────────────────────┘               │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

### 缺陷修复

作为首个正式版本，我们在内测阶段修复了大量问题：

1. 修复并发评测时的超时问题，优化为多 worker 并行
2. 修复 embeddings 模型配置加载失败的问题
3. 修复评测进度更新不实时的问题
4. 修复长文本评测导致的 token 超限问题

### 优化

1. **评测速度大幅提升**：通过并行优化，10 个样本的评测时间从 8 分钟降低到 2-3 分钟
2. **支持多种 LLM 提供商**：OpenAI、SiliconFlow、DeepSeek、智谱 AI 等
3. **完善的错误处理**：评测失败时提供详细的错误诊断信息
4. **数据持久化**：所有评测结果永久保存，支持历史对比

本版本同时提供了 **Docker 一键部署方案**，3 分钟即可完成部署。详情可参考[官方文档](https://github.com/KnowFlowRAG/KnowEval)。

---

## 产品细节

### 一、五维度评测体系：从主观判断到量化分析

传统的 RAG 评测依赖人工抽查，效率低且主观性强。KnowEval 基于 **RAGAS 框架**，提供了五大评测维度：

#### 1. 忠实度 (Faithfulness)

**评测目标**：答案是否基于检索到的上下文，而非 LLM 的内部知识或"幻觉"。

**技术实现**：

- 将答案拆解为多个陈述句 (statements)
- 逐条判断每个陈述是否能在上下文中找到依据
- 计算有依据的陈述占比

**应用场景**：金融、法律等对事实准确性要求极高的领域。

```python
# 示例评测逻辑
faithfulness = supported_statements / total_statements

# 示例结果
# 问题："KnowFlow 支持哪些文档格式？"
# 上下文："KnowFlow 支持 PDF、Word、Excel、PPT 等格式。"
# 回答："KnowFlow 支持 PDF、Word 和 Markdown 格式。"
#
# 分析：
# ✅ "支持 PDF" - 有依据
# ✅ "支持 Word" - 有依据
# ❌ "支持 Markdown" - 无依据（上下文未提及）
#
# 忠实度得分：2/3 = 66.7%
```

#### 2. 答案正确性 (Answer Correctness)

**评测目标**：对比标准答案，综合评估语义相似度和事实准确性。

**技术实现**：

- **F1 Score**：通过 LLM 判断答案中的陈述是否与标准答案匹配
- **Similarity Score**：通过 Embeddings 计算语义相似度
- 加权平均得到最终得分

**应用场景**：有明确标准答案的场景，如产品 FAQ、操作手册等。

```python
# 计算公式
answer_correctness = α × f1_score + β × similarity_score

# 示例结果
# 标准答案："喷砂钢材表面可溶性氯化物含量应不大于 7 μg/cm²。"
# 实际答案："喷砂钢材表面可溶性氯化物含量应不大于 7μg/cm²。超标时应采用高压淡水冲洗。"
#
# F1 Score: 0.95 (关键信息准确)
# Similarity: 0.88 (语义高度相似)
# Answer Correctness: 0.92
```

#### 3. 上下文精确度 (Context Precision)

**评测目标**：检索到的文档片段是否与问题高度相关。

**技术实现**：

- 逐个判断检索到的上下文是否与问题相关
- 计算相关上下文的排序质量 (类似 NDCG)
- 越相关的内容排名越靠前，得分越高

**应用场景**：优化检索算法，减少无关文档干扰。

#### 4. 上下文召回率 (Context Recall)

**评测目标**：是否检索到回答问题所需的所有关键信息。

**技术实现**：

- 提取标准答案中的关键信息点
- 判断这些信息点是否都能在检索到的上下文中找到
- 计算覆盖率

**应用场景**：诊断检索遗漏问题，优化 Top-K 参数。

#### 5. 答案相关性 (Answer Relevancy)

**评测目标**：答案是否切题，没有包含冗余或无关信息。

**技术实现**：

- 基于答案反向生成可能的问题
- 对比生成的问题与原问题的相似度
- 相似度越高，说明答案越切题

**应用场景**：优化 Prompt，避免答案啰嗦或跑题。

---

### 二、AI 智能生成测试集：从 0 到 1 的突破

手动构建测试数据集是 RAG 评测的最大瓶颈。KnowEval 提供了 **AI 自动生成测试集** 功能，彻底解决这一痛点。

#### 工作流程

```
┌─────────────────────────────────────────────────────────┐
│              AI 生成测试集工作流                          │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  ①  选择知识库                                           │
│      └─▶ 连接 KnowFlow/RAGFlow                          │
│                                                          │
│  ②  配置生成参数                                         │
│      ├─▶ 生成数量 (1-100)                               │
│      ├─▶ 问题类型 (事实型/推理型/对比型)                  │
│      └─▶ 难度等级 (简单/中等/困难)                       │
│                                                          │
│  ③  AI 智能生成                                          │
│      ├─▶ 从知识库提取文档片段                            │
│      ├─▶ 基于片段生成自然问题                            │
│      ├─▶ 提取标准答案                                    │
│      └─▶ 关联参考上下文                                  │
│                                                          │
│  ④  人工审核（可选）                                     │
│      └─▶ 预览、编辑、删除                                │
│                                                          │
│  ⑤  一键导入评测                                         │
│      └─▶ 立即开始评测                                    │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

#### 技术亮点

1. **智能问题生成**：基于文档语义自动生成多样化问题

   - 事实型："KnowFlow 支持哪些切块方法？"
   - 推理型："为什么 Title 切块适合论文场景？"
   - 对比型："MinerU 和 Dots 有什么区别？"
2. **自动提取标准答案**：从上下文中智能提取答案片段
3. **上下文关联**：自动关联问题与相关文档片段
4. **批量生成**：支持一次生成 10-100 个测试样本

#### 实际案例

某金融科技公司使用 KnowFlow 构建了产品知识库，包含 500+ 篇文档。使用 KnowEval 的 AI 生成功能：

- **10 分钟生成 50 个测试样本**（人工构建需要 2-3 天）
- **覆盖率达 85%**（涵盖了大部分常见问题场景）
- **准确率达 90%**（仅需微调 5 个样本）

---

### 三、可视化评测报告：让数据说话

评测数据的价值在于洞察。KnowEval 提供了强大的可视化分析能力：

#### 1. 评测仪表盘

**核心指标一目了然**：

- 知识库评测进度：75/100
- 总评测次数：7 次
- 活跃数据集：5 个
- 平均耗时：6.1 分钟

**五维度雷达图**：

- 忠实度：84.4%
- 答案正确性：67.9%
- 上下文精确度：55.2%
- 上下文召回率：100%
- 答案相关性：43.8%

通过雷达图，可以直观发现：

- ✅ 上下文召回率满分 → 检索范围足够
- ⚠️ 上下文精确度偏低 → 检索到了一些无关文档
- ⚠️ 答案相关性偏低 → 答案可能包含冗余信息

**优化建议**：

1. 调整检索算法，提高精确度（如增加 rerank）
2. 优化 Prompt，让答案更简洁精准

#### 2. 任务管理

![](https://fastly.jsdelivr.net/gh/bucketio/img6@main/2025/11/10/1762763338657-119adfba-88ed-4558-8270-28131a3df554.png)

- 实时进度追踪：当前任务 65% 完成
- 历史记录查询：查看过往评测结果
- 一键创建评测：选择数据集和指标即可开始

#### 3. 详细报告

**单条问答诊断**：

- 问题原文
- RAG 系统回答
- 标准答案
- 检索到的上下文
- 各项指标详细得分
- 错误原因分析

**导出功能**：

- JSON 格式：用于程序化分析
- Excel 报表：便于团队分享讨论

---

### 四、对接 KnowFlow：打造数据治理闭环

KnowEval 与 KnowFlow 深度集成，形成完整的 RAG 工程化闭环。

#### 典型工作流

```
第 1 步：数据治理（KnowFlow）
   ├─▶ 使用 MinerU/Dots 解析文档
   ├─▶ 选择切块方法（Smart/Title/Parent-Child）
   ├─▶ 配置 Embedding 模型
   └─▶ 构建知识库

第 2 步：应用开发
   ├─▶ 开发 RAG 应用
   ├─▶ 配置检索参数
   └─▶ 设计 Prompt 模板

第 3 步：质量评测（KnowEval）
   ├─▶ AI 生成测试集
   ├─▶ 运行五维度评测
   ├─▶ 分析可视化报告
   └─▶ 发现优化方向

第 4 步：持续优化
   ├─▶ 方案A：调整切块参数（如增加父标题）
   ├─▶ 方案B：优化检索算法（如增加 rerank）
   ├─▶ 方案C：改进 Prompt 模板
   └─▶ 方案D：升级 Embedding 模型

第 5 步：A/B 测试（KnowEval）
   ├─▶ 对比多个方案的评测结果
   ├─▶ 量化分析提升效果
   └─▶ 选择最优方案

第 6 步：一键应用
   └─▶ 将最优配置应用到生产环境
```

#### 实际案例：某法律科技公司的优化之路

**背景**：

- 使用 KnowFlow 构建法律文书知识库（2000+ 文档）
- RAG 应用上线后，用户反馈答案不够准确

**优化过程**：

**第一轮评测**（基线方案）：

```
方案：General 切块 + BGE-Large Embedding
评测结果：
  - 忠实度：75%
  - 答案正确性：62%
  - 上下文精确度：58%
  - 上下文召回率：85%
  - 答案相关性：55%
```

**问题诊断**：

- 忠实度偏低 → 答案包含"幻觉"
- 上下文精确度偏低 → 检索到无关文档

**第二轮优化**（调整切块方法）：

```
方案：Title 切块（按 H2 标题）+ 增加父标题
评测结果：
  - 忠实度：82% ↑
  - 答案正确性：68% ↑
  - 上下文精确度：71% ↑↑
  - 上下文召回率：88% ↑
  - 答案相关性：58% ↑
```

**第三轮优化**（增加 Rerank）：

```
方案：Title 切块 + 父标题 + BGE-Reranker
评测结果：
  - 忠实度：88% ↑
  - 答案正确性：76% ↑
  - 上下文精确度：85% ↑↑
  - 上下文召回率：92% ↑
  - 答案相关性：65% ↑
```

**最终效果**：

- 整体评分从 67% 提升到 81%（**+14%**）
- 用户投诉率下降 60%
- 优化周期从 2 个月缩短到 2 周

---

## 技术实现原理

### 一、评测引擎架构

KnowEval 基于 **RAGAS（RAG Assessment）** 框架构建，采用 LLM-as-a-Judge 的评测范式。

#### 核心架构

```python
┌─────────────────────────────────────────────────────────┐
│                    KnowEval 架构                         │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  ┌──────────────────────────────────────────┐           │
│  │         评测任务调度层                     │           │
│  │  • 任务队列管理                            │           │
│  │  • 并发控制（8 workers）                  │           │
│  │  • 进度追踪与回调                          │           │
│  └────────────────┬─────────────────────────┘           │
│                   │                                      │
│  ┌────────────────▼─────────────────────────┐           │
│  │         RAGAS 评测引擎                     │           │
│  │  ┌──────────────────────────────────┐    │           │
│  │  │  Faithfulness Evaluator          │    │           │
│  │  │  • Statement Extraction (LLM)    │    │           │
│  │  │  • Verification (LLM)            │    │           │
│  │  └──────────────────────────────────┘    │           │
│  │  ┌──────────────────────────────────┐    │           │
│  │  │  Answer Correctness Evaluator    │    │           │
│  │  │  • F1 Score (LLM)                │    │           │
│  │  │  • Similarity (Embeddings)       │    │           │
│  │  └──────────────────────────────────┘    │           │
│  │  ┌──────────────────────────────────┐    │           │
│  │  │  Context Precision Evaluator     │    │           │
│  │  │  • Relevance Judgment (LLM)      │    │           │
│  │  └──────────────────────────────────┘    │           │
│  │  └── ... (其他评测器)                     │           │
│  └────────────────┬─────────────────────────┘           │
│                   │                                      │
│  ┌────────────────▼─────────────────────────┐           │
│  │         LLM 服务层                         │           │
│  │  • OpenAI API                             │           │
│  │  • SiliconFlow API                        │           │
│  │  • DeepSeek API                           │           │
│  │  • 智谱 AI API                            │           │
│  └────────────────┬─────────────────────────┘           │
│                   │                                      │
│  ┌────────────────▼─────────────────────────┐           │
│  │         数据存储层                         │           │
│  │  • SQLite (评测结果)                      │           │
│  │  • JSON (详细报告)                        │           │
│  │  • 文件系统 (数据集)                      │           │
│  └──────────────────────────────────────────┘           │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

#### 性能优化

**1. 并行评测**

原始实现中，每个样本的评测需要 3-4 次 LLM 调用，串行执行非常慢。我们通过以下优化将速度提升 4 倍：

```python
# 优化前：串行执行
for sample in samples:
    result = evaluate(sample)  # 40-50秒/样本
# 10个样本需要 8-9 分钟

# 优化后：并行执行
run_config = RunConfig(
    max_workers=8,      # 并行 8 个 worker
    max_retries=3,      # 失败重试 3 次
    timeout=600         # 单个任务超时 10 分钟
)
results = evaluate(dataset, run_config=run_config)
# 10个样本仅需 2-3 分钟
```

**2. 智能缓存**

对于相同的问题和上下文，LLM 的评测结果是确定的。我们实现了结果缓存机制：

```python
# 计算缓存键
cache_key = hash(question + context + answer + metric_name)

# 命中缓存直接返回
if cache_key in cache:
    return cache[cache_key]

# 否则调用 LLM 并缓存结果
result = llm_evaluate(...)
cache[cache_key] = result
return result
```

**3. 超时重试**

为了应对 API 限流和网络波动，实现了指数退避重试机制：

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=60)
)
async def call_llm_api(prompt):
    response = await client.chat.completions.create(...)
    return response
```

### 二、AI 生成测试集的技术细节

#### 生成流程

```python
步骤 1：文档采样
   ├─▶ 从知识库随机抽取文档片段
   ├─▶ 过滤过短/过长的片段
   └─▶ 确保覆盖不同主题

步骤 2：问题生成（LLM）
   ├─▶ Prompt: "基于以下文档，生成 {n} 个自然问题"
   ├─▶ 约束条件：
   │    ├─ 问题类型（事实型/推理型/对比型）
   │    ├─ 难度等级（简单/中等/困难）
   │    └─ 问题长度（10-50字）
   └─▶ 生成多样化问题

步骤 3：答案提取（LLM）
   ├─▶ Prompt: "从以下文档中提取问题的答案"
   ├─▶ 约束条件：
   │    ├─ 答案必须基于文档
   │    ├─ 答案长度（20-200字）
   │    └─ 答案格式（文本/列表/表格）
   └─▶ 提取标准答案

步骤 4：上下文关联
   ├─▶ 将问题与原始文档片段关联
   ├─▶ 添加参考上下文（reference_contexts）
   └─▶ 构建完整测试样本

步骤 5：质量检查
   ├─▶ 过滤过于简单的问题（如"是什么"）
   ├─▶ 去重相似问题
   └─▶ 人工审核（可选）
```

#### Prompt 示例

```python
QUESTION_GENERATION_PROMPT = """
你是一个专业的问题生成器。请基于以下文档内容，生成 {count} 个高质量的问题。

文档内容：
{document}

要求：
1. 问题类型：{question_type}
   - 事实型：询问具体信息（如"KnowFlow支持哪些格式？"）
   - 推理型：需要理解和推理（如"为什么Title切块适合论文？"）
   - 对比型：比较不同概念（如"MinerU和Dots有什么区别？"）

2. 难度等级：{difficulty}
   - 简单：答案可直接在文档中找到
   - 中等：需要理解多个段落
   - 困难：需要综合分析和推理

3. 问题长度：10-50字
4. 确保问题自然、具体、有意义
5. 避免过于简单的问题（如"这是什么？"）

请以 JSON 格式返回，格式如下：
[
  {{"question": "问题1"}},
  {{"question": "问题2"}},
  ...
]
"""
```

### 三、多 LLM 提供商适配

KnowEval 支持主流 LLM 提供商，通过统一的接口层实现无缝切换：

```python
# 统一配置接口
class LLMConfig:
    provider: str        # openai/siliconflow/deepseek/zhipu
    model: str          # 模型名称
    api_key: str        # API 密钥
    base_url: str       # API 端点
    temperature: float  # 温度参数

# LLM 适配器
class LLMAdapter:
    def __init__(self, config: LLMConfig):
        if config.provider == "openai":
            self.client = OpenAI(api_key=config.api_key)
        elif config.provider == "siliconflow":
            self.client = OpenAI(
                api_key=config.api_key,
                base_url="https://api.siliconflow.cn/v1"
            )
        # ... 其他提供商

    async def chat_completion(self, messages):
        response = await self.client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature
        )
        return response.choices[0].message.content
```

---

## 未来展望

KnowEval v1.0.0 是我们在 RAG 工程化道路上的重要里程碑。通过将评测能力产品化，我们让 **"凭感觉调参"** 变成了 **"数据驱动优化"**。

基于与客户的深度交流，KnowEval 的产品定位也越来越清晰：**让 RAG 系统的质量可量化、可优化、可持续，将评测能力转化为核心竞争力。**

后续我们将围绕以下方向持续迭代：

### 近期规划（Q1-Q2 2025）

**1. 更多评测指标**

- **答案完整性**：评估答案是否完整覆盖问题的所有方面
- **答案简洁度**：评估答案是否冗长啰嗦
- **引用准确性**：评估答案中的引用是否正确标注来源
- **多轮对话评测**：支持评测多轮对话场景

**2. 对比评测功能**

- A/B 测试：同时评测多个方案，可视化对比
- 历史趋势：追踪优化过程中的指标变化
- 基线对比：与行业基准进行横向对比

**3. 自动化优化建议**

- 智能诊断：自动分析评测结果，发现问题根因
- 优化建议：基于评测数据，给出具体优化方案
- 一键应用：将优化方案直接应用到 KnowFlow

**4. 更多 RAG 系统集成**

- LangChain：支持 LangChain 应用评测
- LlamaIndex：支持 LlamaIndex 应用评测
- 自定义 API：支持任意 RAG 系统通过 API 接入

### 中期规划（Q3-Q4 2025）

**1. 团队协作能力**

- 多人协作：支持团队共享评测数据集和结果
- 权限管理：细粒度的角色和权限控制
- 评论讨论：支持在评测结果上添加评论和讨论

**2. 持续监控**

- 生产环境监控：实时监控线上 RAG 系统质量
- 告警机制：质量下降时自动告警
- 定时评测：定期自动运行评测任务

**3. 自定义评测指标**

- 插件机制：支持用户自定义评测指标
- DSL 配置：通过配置文件定义评测逻辑
- Python SDK：提供 SDK 方便集成到 CI/CD

**4. 企业级能力**

- 私有化部署：支持企业内网部署
- SSO 单点登录：集成企业身份认证系统
- 审计日志：完整的操作审计追踪

### 长期愿景

**构建 RAG 质量保证的行业标准**

我们相信，随着 RAG 应用的普及，**质量评测将成为 RAG 工程化的核心基础设施**。KnowEval 的愿景是：

1. **制定评测标准**：推动 RAG 评测指标的标准化
2. **建设基准测试**：构建覆盖多个行业的公开基准数据集
3. **打造生态闭环**：与 KnowFlow 深度集成，形成从数据治理到质量保证的完整生态

---

## 开源与社区

KnowEval 已开源并托管在 GitHub：

**🔗 项目地址**：https://github.com/KnowFlowRAG/KnowEval

**📄 开源协议**：AGPL-3.0

### 快速开始

Docker 一键部署（推荐）：

```bash
# 1. 克隆项目
git clone https://github.com/KnowFlowRAG/KnowEval.git
cd KnowEval/docker-deploy

# 2. 配置环境变量
cp .env.example .env
# 编辑 .env，配置 RAGFlow API 和 LLM API

# 3. 启动服务
./start.sh

# ✅ 访问 http://localhost:5003
```

### 加入社区

我们非常欢迎大家参与到 KnowEval 的建设中：

- **使用反馈**：告诉我们您的使用体验和改进建议
- **Bug 报告**：发现问题请提 Issue
- **功能建议**：期待您的 Feature Request
- **代码贡献**：欢迎提交 Pull Request

**扫描下方二维码，关注公众号 「KnowFlow 企业知识库」，加入内部交流群，学习和分享 RAG 工程化经验。**

---

## 结语

在 AI 时代，**数据质量决定了应用质量**。KnowFlow 解决了数据治理的问题，让知识库"可信"；KnowEval 解决了质量评测的问题，让 RAG 系统"可控"。

**数据治理 + 质量保证 = RAG 工程化的完整闭环**

我们相信，通过 KnowFlow 生态的持续建设，可以帮助企业：

1. **降低 RAG 应用的开发门槛**：从数据准备到质量保证，全流程工具化
2. **提升 RAG 应用的质量上限**：通过数据驱动优化，持续提升问答效果
3. **加速 RAG 应用的落地速度**：从几个月缩短到几周甚至几天

这就是我们的使命：**将结构化与非结构化数据治理成对大模型更可信的输入，构建面向未来的数据治理平台，重塑 AI 时代的数据根基。**

期待与您一起，推动 RAG 工程化的发展！
